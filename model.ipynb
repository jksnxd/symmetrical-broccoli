{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\silve\\Documents\\ex12\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Dataset.from_pandas(pd.read_parquet('tokenized_dataset.parquet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = df.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\silve\\AppData\\Local\\Temp\\ipykernel_16676\\4207227866.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    eval_strategy=\"epoch\",     \n",
    "    learning_rate=2e-5,              \n",
    "    per_device_train_batch_size=8,  \n",
    "    per_device_eval_batch_size=8,    \n",
    "    num_train_epochs=3,             \n",
    "    weight_decay=0.01,               \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,           \n",
    "    tokenizer=tokenizer,                 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 33%|███▎      | 149/447 [01:04<01:47,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6464056968688965, 'eval_runtime': 4.4835, 'eval_samples_per_second': 66.466, 'eval_steps_per_second': 8.476, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|██████▋   | 298/447 [02:08<00:54,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6035328507423401, 'eval_runtime': 4.4305, 'eval_samples_per_second': 67.26, 'eval_steps_per_second': 8.577, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 447/447 [03:13<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6458290219306946, 'eval_runtime': 4.4526, 'eval_samples_per_second': 66.927, 'eval_steps_per_second': 8.534, 'epoch': 3.0}\n",
      "{'train_runtime': 193.1944, 'train_samples_per_second': 18.463, 'train_steps_per_second': 2.314, 'train_loss': 0.5984364622657998, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:04<00:00,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.6458290219306946, 'eval_runtime': 4.4745, 'eval_samples_per_second': 66.6, 'eval_steps_per_second': 8.493, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\silve\\AppData\\Local\\Temp\\ipykernel_16676\\3474840007.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      " 12%|█▎        | 75/600 [04:26<23:52,  2.73s/it]\n",
      " 12%|█▎        | 75/600 [04:31<23:52,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6615409851074219, 'eval_runtime': 4.9366, 'eval_samples_per_second': 60.365, 'eval_steps_per_second': 3.849, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 150/600 [09:01<18:35,  2.48s/it]\n",
      " 25%|██▌       | 150/600 [09:06<18:35,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7700636982917786, 'eval_runtime': 4.4569, 'eval_samples_per_second': 66.862, 'eval_steps_per_second': 4.263, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 225/600 [12:38<12:58,  2.08s/it]\n",
      " 38%|███▊      | 225/600 [12:42<12:58,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8230057954788208, 'eval_runtime': 4.4117, 'eval_samples_per_second': 67.548, 'eval_steps_per_second': 4.307, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 50%|█████     | 300/600 [16:23<10:22,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9752998352050781, 'eval_runtime': 4.3717, 'eval_samples_per_second': 68.165, 'eval_steps_per_second': 4.346, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 62%|██████▎   | 375/600 [20:01<08:02,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0607362985610962, 'eval_runtime': 4.5975, 'eval_samples_per_second': 64.817, 'eval_steps_per_second': 4.133, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|███████▌  | 450/600 [24:46<05:10,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1187536716461182, 'eval_runtime': 4.3835, 'eval_samples_per_second': 67.983, 'eval_steps_per_second': 4.334, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 500/600 [27:31<06:08,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1688, 'grad_norm': 5.535717964172363, 'learning_rate': 1.6666666666666667e-06, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 88%|████████▊ | 525/600 [28:59<03:07,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.180843710899353, 'eval_runtime': 4.7093, 'eval_samples_per_second': 63.279, 'eval_steps_per_second': 4.035, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 600/600 [33:37<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1789630651474, 'eval_runtime': 4.7237, 'eval_samples_per_second': 63.086, 'eval_steps_per_second': 4.022, 'epoch': 8.0}\n",
      "{'train_runtime': 2017.5873, 'train_samples_per_second': 4.715, 'train_steps_per_second': 0.297, 'train_loss': 0.1517018469174703, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:04<00:00,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 1.1789630651474, 'eval_runtime': 4.7296, 'eval_samples_per_second': 63.008, 'eval_steps_per_second': 4.017, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    eval_strategy=\"epoch\",     \n",
    "    learning_rate=1e-5,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=16,    \n",
    "    num_train_epochs=8,             \n",
    "    weight_decay=0.001,               \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,           \n",
    "    tokenizer=tokenizer,                 \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
